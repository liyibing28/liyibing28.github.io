---
layout: post
title: CMFL: Mitigating Communication Overhead for Federated Learning
category: 科研
tags: FederatedLearning 联邦学习 论文 笔记
keywords: 
description:
---

## 方法

由于移动设备通常具有有限的数据计划，并且与维护全局模型的中央服务器之间的网络连接速度较慢，因此减轻通信开销至关重要。虽然现有的工作主要集中在通过数据压缩来减少每次更新中传输的总位，但是我们研究了一种正交方法，该方法可以识别客户端进行的不相关的更新，并阻止它们上载以减少网络占用空间。遵循这个想法，我们在本文中提出了缓解交流的联合学习（CMFL）。 CMFL向客户提供有关模型更新的全球趋势的反馈信息。每个客户都检查其更新是否符合这种全球趋势，并且是否足够相关以进行改进建模。通过避免将那些不相关的更新上载到服务器，CMFL可以大大减少通信开销，同时仍然保证学习的收敛性。

### 减少训练期间通信开销的方法

+ 减少每个客户端更新传输的总位数
+ 减少每个客户端更新传输的总数。

### 相关论文

K. Hsieh, A. Harlap, N. Vijaykumar, D. Konomis, G. R. Ganger, P. B. Gibbons, and O. Mutlu, “Gaia: Geo-distributed machine learning approaching lan speeds.” in NSDI, 2017, pp. 629–647.

+ 该提案提出了通过从数据传输中排除无用的更新来最小化地理分布式机器学习的通信开销的方法。 但是，Gaia不会检查局部更新是否与全局收敛相关，而是通过将其绝对值（幅度）与预定义的阈值进行比较来关注局部更新的重要性：如果其幅度大于阈值，则认为该更新是重要的。 大小不重要的本地更新不会上传，以减少网络占用量。尽管事实证明Gaia对于跨少量数据中心的地理分布式学习有效，但由于存在许多不匹配，它无法很好地转移到FL设置。特别地，基于幅度的重要性度量仅涉及模型训练的速度，而与优化方向无关。 通过简单地比较具有固定阈值的更新的重要性，Gaia无法确定此更新（它是一个局部梯度）是否与所有客户端上的协作优化趋势一致，因此无法正确识别其（无关紧要）的相关性。 考虑到参与FL的大量客户端（例如，成千上万的边缘设备），该问题甚至会更加突出。

F. Chen, Z. Dong, Z. Li, and X. He, “Federated meta-learning for recommendation,” arXiv preprint arXiv:1802.07876, 2018.

在这种方式下，用户信息是在算法级别上共享的，而不是先前方法中假设的模型或数据。 尽管这些学习框架可以加快培训过程，但在减少通信开销方面几乎没有改善

+ 可以加快

### 基本过程

在每次学习迭代中，客户端首先从中央服务器接收关于全局更新的反馈信息。接下来，客户进行本地培训并生成本地更新。然后，客户端将其与全局更新进行比较，检查两个渐变的对齐程度。为此，在进行本地更新的情况下，CMFL会计算其参数具有不同符号（正号或负号）的百分比与全局更新中其对应参数的百分比。直观地讲，百分比越高，本地更新与协作融合的分歧就越大，因此它就越不相关。通过将不相关的本地更新排除在数据上传之外，CMFL可以有效减少FL的通信开销，同时仍提供可证明的收敛保证

### 为什么一些局部更新对全局更新无效

#### 如何度量

我们通过称为标准化模型散度的度量标准来度量全局参数和局部参数之间的差异
针对每个模型参数x_j 将归一化模型散度定义为其值在客户端和通过全局值归一化的全局模型之间的平均差

+ 比较本地跟新中符号和全局更新中相同的比例

### 难点

+ 丢弃了部分更新，如何证明对收敛性的影响
+ 如何度量差异大小
  + 利用上一次更新的结果

### Communication-Mitigated Federated Learnin

#### Aggregation at the central server.

#### Local optimization on client-side devices.

### 实验

EC2 仿真
  30-machine Amazon EC2 simulation
